{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"wangrui6/Zhihu-KOL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "data_dict['question'] = ds['train']['INSTRUCTION']\n",
    "data_dict['answer'] = ds['train']['RESPONSE']\n",
    "for key in json.loads(ds['train']['METADATA'][0]).keys():\n",
    "    data_dict[key] = [json.loads(x)[key] for x in ds['train']['METADATA']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006218\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>url</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>answer_creation_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>怎么说服男朋友买烤箱？</td>\n",
       "      <td>emmmmm，首先想说的是，我买厨房用品一般是不用「说服」的，只是在厨房堆的满满当当的情况下...</td>\n",
       "      <td>357137111.0</td>\n",
       "      <td>9.143328e+08</td>\n",
       "      <td>https://www.zhihu.com/question/357137111/answe...</td>\n",
       "      <td>赞同 15</td>\n",
       "      <td>2019-11-28T12:01:22.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>航天从业者是如何看待电视剧《你是我的荣耀》的？</td>\n",
       "      <td>难得有个关于航天的剧，职场情节悬不悬浮，航天设定和细节走不走心？带着放大镜看了前18集，...</td>\n",
       "      <td>475169837.0</td>\n",
       "      <td>2.053313e+09</td>\n",
       "      <td>https://www.zhihu.com/question/475169837/answe...</td>\n",
       "      <td>赞同 4432</td>\n",
       "      <td>2021-08-11T07:26:08.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>如何看待PayPal正式进入中国？</td>\n",
       "      <td>PayPal不仅是美国支付巨头，也是国际支付巨头，目前已开拓全球200多个市场，美国以外的市...</td>\n",
       "      <td>348551037.0</td>\n",
       "      <td>8.661034e+08</td>\n",
       "      <td>https://www.zhihu.com/question/348551037/answe...</td>\n",
       "      <td>赞同 127</td>\n",
       "      <td>2019-10-22T09:11:15.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>中金公司交易员月薪八万五是如何做到的？</td>\n",
       "      <td>1、首先，考虑到这位交易员的工作经验，月薪八万五的表述是不正确的：其实是一年的全部薪酬除以1...</td>\n",
       "      <td>545938899.0</td>\n",
       "      <td>2.602364e+09</td>\n",
       "      <td>https://www.zhihu.com/question/545938899/answe...</td>\n",
       "      <td>赞同 450</td>\n",
       "      <td>2022-07-31T13:29:04.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>摇滚乐（金属）给你们带来了什么？</td>\n",
       "      <td>ㄟ( ▔, ▔ )ㄏ哪里带来了什么东西啊，除了找到热爱的东西，也失去了很多。听重型现场像疯子...</td>\n",
       "      <td>361437216.0</td>\n",
       "      <td>1.073541e+09</td>\n",
       "      <td>https://www.zhihu.com/question/361437216/answe...</td>\n",
       "      <td>赞同 5</td>\n",
       "      <td>2020-03-12T05:49:28.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  question                                             answer  \\\n",
       "0              怎么说服男朋友买烤箱？  emmmmm，首先想说的是，我买厨房用品一般是不用「说服」的，只是在厨房堆的满满当当的情况下...   \n",
       "1  航天从业者是如何看待电视剧《你是我的荣耀》的？    难得有个关于航天的剧，职场情节悬不悬浮，航天设定和细节走不走心？带着放大镜看了前18集，...   \n",
       "2        如何看待PayPal正式进入中国？  PayPal不仅是美国支付巨头，也是国际支付巨头，目前已开拓全球200多个市场，美国以外的市...   \n",
       "3      中金公司交易员月薪八万五是如何做到的？  1、首先，考虑到这位交易员的工作经验，月薪八万五的表述是不正确的：其实是一年的全部薪酬除以1...   \n",
       "4         摇滚乐（金属）给你们带来了什么？  ㄟ( ▔, ▔ )ㄏ哪里带来了什么东西啊，除了找到热爱的东西，也失去了很多。听重型现场像疯子...   \n",
       "\n",
       "   question_id     answer_id  \\\n",
       "0  357137111.0  9.143328e+08   \n",
       "1  475169837.0  2.053313e+09   \n",
       "2  348551037.0  8.661034e+08   \n",
       "3  545938899.0  2.602364e+09   \n",
       "4  361437216.0  1.073541e+09   \n",
       "\n",
       "                                                 url  upvotes  \\\n",
       "0  https://www.zhihu.com/question/357137111/answe...    赞同 15   \n",
       "1  https://www.zhihu.com/question/475169837/answe...  赞同 4432   \n",
       "2  https://www.zhihu.com/question/348551037/answe...   赞同 127   \n",
       "3  https://www.zhihu.com/question/545938899/answe...   赞同 450   \n",
       "4  https://www.zhihu.com/question/361437216/answe...     赞同 5   \n",
       "\n",
       "       answer_creation_time  \n",
       "0  2019-11-28T12:01:22.000Z  \n",
       "1  2021-08-11T07:26:08.000Z  \n",
       "2  2019-10-22T09:11:15.000Z  \n",
       "3  2022-07-31T13:29:04.000Z  \n",
       "4  2020-03-12T05:49:28.000Z  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.DataFrame(data_dict)\n",
    "print(df_train.shape[0])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159007"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df_train['question_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1006218"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df_train['answer_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>url</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>answer_creation_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>怎么说服男朋友买烤箱？</td>\n",
       "      <td>emmmmm，首先想说的是，我买厨房用品一般是不用「说服」的，只是在厨房堆的满满当当的情况下...</td>\n",
       "      <td>357137111</td>\n",
       "      <td>914332816</td>\n",
       "      <td>https://www.zhihu.com/question/357137111/answe...</td>\n",
       "      <td>15</td>\n",
       "      <td>2019-11-28T12:01:22.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>航天从业者是如何看待电视剧《你是我的荣耀》的？</td>\n",
       "      <td>难得有个关于航天的剧，职场情节悬不悬浮，航天设定和细节走不走心？带着放大镜看了前18集，...</td>\n",
       "      <td>475169837</td>\n",
       "      <td>2053313113</td>\n",
       "      <td>https://www.zhihu.com/question/475169837/answe...</td>\n",
       "      <td>4432</td>\n",
       "      <td>2021-08-11T07:26:08.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>如何看待PayPal正式进入中国？</td>\n",
       "      <td>PayPal不仅是美国支付巨头，也是国际支付巨头，目前已开拓全球200多个市场，美国以外的市...</td>\n",
       "      <td>348551037</td>\n",
       "      <td>866103409</td>\n",
       "      <td>https://www.zhihu.com/question/348551037/answe...</td>\n",
       "      <td>127</td>\n",
       "      <td>2019-10-22T09:11:15.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>中金公司交易员月薪八万五是如何做到的？</td>\n",
       "      <td>1、首先，考虑到这位交易员的工作经验，月薪八万五的表述是不正确的：其实是一年的全部薪酬除以1...</td>\n",
       "      <td>545938899</td>\n",
       "      <td>2602363788</td>\n",
       "      <td>https://www.zhihu.com/question/545938899/answe...</td>\n",
       "      <td>450</td>\n",
       "      <td>2022-07-31T13:29:04.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>摇滚乐（金属）给你们带来了什么？</td>\n",
       "      <td>ㄟ( ▔, ▔ )ㄏ哪里带来了什么东西啊，除了找到热爱的东西，也失去了很多。听重型现场像疯子...</td>\n",
       "      <td>361437216</td>\n",
       "      <td>1073541478</td>\n",
       "      <td>https://www.zhihu.com/question/361437216/answe...</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-03-12T05:49:28.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  question                                             answer  \\\n",
       "0              怎么说服男朋友买烤箱？  emmmmm，首先想说的是，我买厨房用品一般是不用「说服」的，只是在厨房堆的满满当当的情况下...   \n",
       "1  航天从业者是如何看待电视剧《你是我的荣耀》的？    难得有个关于航天的剧，职场情节悬不悬浮，航天设定和细节走不走心？带着放大镜看了前18集，...   \n",
       "2        如何看待PayPal正式进入中国？  PayPal不仅是美国支付巨头，也是国际支付巨头，目前已开拓全球200多个市场，美国以外的市...   \n",
       "3      中金公司交易员月薪八万五是如何做到的？  1、首先，考虑到这位交易员的工作经验，月薪八万五的表述是不正确的：其实是一年的全部薪酬除以1...   \n",
       "4         摇滚乐（金属）给你们带来了什么？  ㄟ( ▔, ▔ )ㄏ哪里带来了什么东西啊，除了找到热爱的东西，也失去了很多。听重型现场像疯子...   \n",
       "\n",
       "   question_id   answer_id                                                url  \\\n",
       "0    357137111   914332816  https://www.zhihu.com/question/357137111/answe...   \n",
       "1    475169837  2053313113  https://www.zhihu.com/question/475169837/answe...   \n",
       "2    348551037   866103409  https://www.zhihu.com/question/348551037/answe...   \n",
       "3    545938899  2602363788  https://www.zhihu.com/question/545938899/answe...   \n",
       "4    361437216  1073541478  https://www.zhihu.com/question/361437216/answe...   \n",
       "\n",
       "   upvotes      answer_creation_time  \n",
       "0       15  2019-11-28T12:01:22.000Z  \n",
       "1     4432  2021-08-11T07:26:08.000Z  \n",
       "2      127  2019-10-22T09:11:15.000Z  \n",
       "3      450  2022-07-31T13:29:04.000Z  \n",
       "4        5  2020-03-12T05:49:28.000Z  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_upvotes(upvote_str):\n",
    "    upvote_str = upvote_str.replace('赞同', '').strip()\n",
    "    if len(upvote_str) > 0:\n",
    "        if \"万\" in upvote_str:\n",
    "            upvote_str = upvote_str.replace('万', '').strip()\n",
    "            return int(float(upvote_str) * 10000)\n",
    "        else:\n",
    "            return int(upvote_str)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "df_train['upvotes'] = df_train['upvotes'].apply(clean_upvotes)\n",
    "df_train['answer_id'] = df_train['answer_id'].astype('int64')\n",
    "df_train['question_id'] = df_train['question_id'].astype('int64')\n",
    "\n",
    "df_train = df_train[df_train['url'].notna()] # use only url is not NaN\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sorted = df_train.sort_values(by='upvotes', ascending=False).head(10000)\n",
    "df_train_sorted.to_csv('../data/zhihu_kol_10k.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 examples to share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sorted = pd.read_csv('../data/zhihu_kol_10k.csv')\n",
    "df_temp = df_train_sorted[['question', 'answer']].head(30)\n",
    "df_temp.to_csv('../data/zhihu_kol_10k_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap question body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sorted = pd.read_csv('../data/zhihu_kol_10k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>url</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>answer_creation_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>长得太美是一种什么样的感觉？</td>\n",
       "      <td>我的亲妹妹，无论到哪儿都有人夸她好看，走在大街上都会被人要微信！！！ 去海南玩的时候随手拍了...</td>\n",
       "      <td>430297692</td>\n",
       "      <td>1715042961</td>\n",
       "      <td>https://www.zhihu.com/question/430297692/answe...</td>\n",
       "      <td>634000</td>\n",
       "      <td>2021-02-04T09:17:34.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>语文考试可以写负能量作文吗？</td>\n",
       "      <td>我真的是服你们这群崽子。 一个个的都是契科夫转世、莫泊桑临凡、欧亨利还魂，茨威格夺舍…… 你...</td>\n",
       "      <td>272068457</td>\n",
       "      <td>1302565494</td>\n",
       "      <td>https://www.zhihu.com/question/272068457/answe...</td>\n",
       "      <td>434000</td>\n",
       "      <td>2020-06-25T15:25:45.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>怎么在一个月内让英语听力有明显的提高？</td>\n",
       "      <td>相信我，看完这篇文章，一个月后你的英语听力水平将会超出你的想象。本篇也是知乎 2018、20...</td>\n",
       "      <td>24706380</td>\n",
       "      <td>387403508</td>\n",
       "      <td>https://www.zhihu.com/question/24706380/answer...</td>\n",
       "      <td>424000</td>\n",
       "      <td>2018-05-09T09:42:29.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>有哪些知识，你知道后你的世界马上就不一样了？</td>\n",
       "      <td>1.原则上可以，就是不可以；原则上不可以，就是可以！ 2.养成吃晚饭就洗碗的习惯，不要泡在水...</td>\n",
       "      <td>38632401</td>\n",
       "      <td>1060250796</td>\n",
       "      <td>https://www.zhihu.com/question/38632401/answer...</td>\n",
       "      <td>399000</td>\n",
       "      <td>2020-03-06T08:44:01.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>如何看透人的心理？</td>\n",
       "      <td>1、朋友圈经常晒自己自拍照的单身女生不能追。   2、“A+姓名+手机号”这种微信名，大多都...</td>\n",
       "      <td>365128026</td>\n",
       "      <td>1426528244</td>\n",
       "      <td>https://www.zhihu.com/question/365128026/answe...</td>\n",
       "      <td>363000</td>\n",
       "      <td>2020-08-22T10:32:08.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>有什么相见恨晚的背单词方法？</td>\n",
       "      <td>谢邀，单词，看这一篇就够了。全文手码5000字，被收藏750000次了，谢谢大家的支持。 我...</td>\n",
       "      <td>48040579</td>\n",
       "      <td>264770860</td>\n",
       "      <td>https://www.zhihu.com/question/48040579/answer...</td>\n",
       "      <td>291000</td>\n",
       "      <td>2017-11-24T01:03:59.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>你手机里最舍不得删的那张照片有什么故事?</td>\n",
       "      <td>这张纸条是我妻子写的。   写下的前24小时:由于已经晚于预产期一周，医生给用我妻子上了催产...</td>\n",
       "      <td>269575911</td>\n",
       "      <td>372282921</td>\n",
       "      <td>https://www.zhihu.com/question/269575911/answe...</td>\n",
       "      <td>273000</td>\n",
       "      <td>2018-04-21T03:41:38.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>人类有没有可能是被设计出来的？</td>\n",
       "      <td>如果是真的，那我一定要找到那个造物主，先把他按在地上一顿爆锤，然后厉声质问他：为什么要在我身...</td>\n",
       "      <td>298688205</td>\n",
       "      <td>528647866</td>\n",
       "      <td>https://www.zhihu.com/question/298688205/answe...</td>\n",
       "      <td>264000</td>\n",
       "      <td>2018-11-10T02:55:33.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>学霸的作息是怎样的，为什么都不会困？</td>\n",
       "      <td>北大15级的来答一发。 学霸不会困，靠的不是体力，是自我管理。 一天分成三个阶段，规律作息，...</td>\n",
       "      <td>267346432</td>\n",
       "      <td>426452693</td>\n",
       "      <td>https://www.zhihu.com/question/267346432/answe...</td>\n",
       "      <td>260000</td>\n",
       "      <td>2018-06-25T16:00:08.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>有哪些读书学不来，却很重要的素质？</td>\n",
       "      <td>（本文所有文字皆为原创，除注明引用外未参考任何文献，谢绝转载，） 书上找不到，也很少有人讨论...</td>\n",
       "      <td>28626263</td>\n",
       "      <td>41992632</td>\n",
       "      <td>https://www.zhihu.com/question/28626263/answer...</td>\n",
       "      <td>259000</td>\n",
       "      <td>2015-03-14T10:48:43.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 question                                             answer  \\\n",
       "0          长得太美是一种什么样的感觉？  我的亲妹妹，无论到哪儿都有人夸她好看，走在大街上都会被人要微信！！！ 去海南玩的时候随手拍了...   \n",
       "1          语文考试可以写负能量作文吗？  我真的是服你们这群崽子。 一个个的都是契科夫转世、莫泊桑临凡、欧亨利还魂，茨威格夺舍…… 你...   \n",
       "2     怎么在一个月内让英语听力有明显的提高？  相信我，看完这篇文章，一个月后你的英语听力水平将会超出你的想象。本篇也是知乎 2018、20...   \n",
       "3  有哪些知识，你知道后你的世界马上就不一样了？  1.原则上可以，就是不可以；原则上不可以，就是可以！ 2.养成吃晚饭就洗碗的习惯，不要泡在水...   \n",
       "4               如何看透人的心理？  1、朋友圈经常晒自己自拍照的单身女生不能追。   2、“A+姓名+手机号”这种微信名，大多都...   \n",
       "5          有什么相见恨晚的背单词方法？  谢邀，单词，看这一篇就够了。全文手码5000字，被收藏750000次了，谢谢大家的支持。 我...   \n",
       "6    你手机里最舍不得删的那张照片有什么故事?  这张纸条是我妻子写的。   写下的前24小时:由于已经晚于预产期一周，医生给用我妻子上了催产...   \n",
       "7         人类有没有可能是被设计出来的？  如果是真的，那我一定要找到那个造物主，先把他按在地上一顿爆锤，然后厉声质问他：为什么要在我身...   \n",
       "8      学霸的作息是怎样的，为什么都不会困？  北大15级的来答一发。 学霸不会困，靠的不是体力，是自我管理。 一天分成三个阶段，规律作息，...   \n",
       "9       有哪些读书学不来，却很重要的素质？  （本文所有文字皆为原创，除注明引用外未参考任何文献，谢绝转载，） 书上找不到，也很少有人讨论...   \n",
       "\n",
       "   question_id   answer_id                                                url  \\\n",
       "0    430297692  1715042961  https://www.zhihu.com/question/430297692/answe...   \n",
       "1    272068457  1302565494  https://www.zhihu.com/question/272068457/answe...   \n",
       "2     24706380   387403508  https://www.zhihu.com/question/24706380/answer...   \n",
       "3     38632401  1060250796  https://www.zhihu.com/question/38632401/answer...   \n",
       "4    365128026  1426528244  https://www.zhihu.com/question/365128026/answe...   \n",
       "5     48040579   264770860  https://www.zhihu.com/question/48040579/answer...   \n",
       "6    269575911   372282921  https://www.zhihu.com/question/269575911/answe...   \n",
       "7    298688205   528647866  https://www.zhihu.com/question/298688205/answe...   \n",
       "8    267346432   426452693  https://www.zhihu.com/question/267346432/answe...   \n",
       "9     28626263    41992632  https://www.zhihu.com/question/28626263/answer...   \n",
       "\n",
       "   upvotes      answer_creation_time  \n",
       "0   634000  2021-02-04T09:17:34.000Z  \n",
       "1   434000  2020-06-25T15:25:45.000Z  \n",
       "2   424000  2018-05-09T09:42:29.000Z  \n",
       "3   399000  2020-03-06T08:44:01.000Z  \n",
       "4   363000  2020-08-22T10:32:08.000Z  \n",
       "5   291000  2017-11-24T01:03:59.000Z  \n",
       "6   273000  2018-04-21T03:41:38.000Z  \n",
       "7   264000  2018-11-10T02:55:33.000Z  \n",
       "8   260000  2018-06-25T16:00:08.000Z  \n",
       "9   259000  2015-03-14T10:48:43.000Z  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "def initialize_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    \n",
    "    service = Service(r\"C:\\Users\\JeffreyCh0\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\")  # Replace with the path to your ChromeDriver\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "# Scrape body content from URL\n",
    "def scrape_body(driver, url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # Wait for the page to load (adjust as needed)\n",
    "        \n",
    "        # Modify the following line to target the specific part of the page you want to scrape\n",
    "        body = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "        return body\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main scraping function\n",
    "def add_body_column(df):\n",
    "    driver = initialize_driver()\n",
    "    bodies = []\n",
    "    for url in df['url']:\n",
    "        print(f\"Scraping: {url}\")\n",
    "        body = scrape_body(driver, url)\n",
    "        bodies.append(body)\n",
    "    driver.quit()\n",
    "    df['body'] = bodies\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.zhihu.com/question/430297692/answer/1715042961'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp['url'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://www.zhihu.com/question/430297692/answer/1715042961\n",
      "Scraping: https://www.zhihu.com/question/272068457/answer/1302565494\n",
      "Scraping: https://www.zhihu.com/question/24706380/answer/387403508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-60e7449e53fd>:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['body'] = bodies\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'进入知乎\\n系统监测到您的网络环境存在异常，为保证您的正常访问，请点击下方验证按钮进行验证。在您验证完成前，该提示将多次出现。\\n开始验证\\n登录知乎\\n·\\n意见反馈'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example DataFrame\n",
    "\n",
    "# Add 'body' column\n",
    "df_temp = add_body_column(df_temp)\n",
    "\n",
    "df_temp['body'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159007\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>怎么说服男朋友买烤箱？</td>\n",
       "      <td>357137111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>航天从业者是如何看待电视剧《你是我的荣耀》的？</td>\n",
       "      <td>475169837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>如何看待PayPal正式进入中国？</td>\n",
       "      <td>348551037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>中金公司交易员月薪八万五是如何做到的？</td>\n",
       "      <td>545938899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>摇滚乐（金属）给你们带来了什么？</td>\n",
       "      <td>361437216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  question  question_id\n",
       "0              怎么说服男朋友买烤箱？    357137111\n",
       "1  航天从业者是如何看待电视剧《你是我的荣耀》的？    475169837\n",
       "2        如何看待PayPal正式进入中国？    348551037\n",
       "3      中金公司交易员月薪八万五是如何做到的？    545938899\n",
       "4         摇滚乐（金属）给你们带来了什么？    361437216"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"wangrui6/Zhihu-KOL\")\n",
    "\n",
    "data_dict = {}\n",
    "data_dict['question'] = ds['train']['INSTRUCTION']\n",
    "for key in json.loads(ds['train']['METADATA'][0]).keys():\n",
    "    if key == \"question_id\":\n",
    "        data_dict[key] = [int(json.loads(x)[key]) for x in ds['train']['METADATA']]\n",
    "\n",
    "df_question = pd.DataFrame(data_dict)\n",
    "del data_dict\n",
    "df_question = df_question.drop_duplicates(subset=['question_id'])\n",
    "print(df_question.shape[0])\n",
    "df_question.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6adfd1be1a4881984a84866cb5c0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emb_model = BGEM3FlagModel('BAAI/bge-m3',  \n",
    "                       use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pre tokenize: 100%|██████████| 622/622 [00:02<00:00, 289.64it/s]\n",
      "Inference Embeddings: 100%|██████████| 622/622 [06:11<00:00,  1.68it/s]\n"
     ]
    }
   ],
   "source": [
    "zh_sentences = df_question['question'].tolist()\n",
    "zh_sentences = list(set(zh_sentences))\n",
    "\n",
    "zh_emb = emb_model.encode(zh_sentences)['dense_vecs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/zhihu_emb.pkl', 'wb') as f:\n",
    "    pickle.dump((zh_sentences,zh_emb), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv('../data/reddit_sample.csv')\n",
    "reddit.columns = [\"question\", \"detail\", \"answer\", \"topic\", \"answer_length\"]\n",
    "reddit_sentences = reddit['question'].tolist()\n",
    "\n",
    "reddit_emb = emb_model.encode(reddit_sentences)['dense_vecs']\n",
    "\n",
    "similarity = reddit_emb @ zh_emb.T\n",
    "\n",
    "top_k = 5\n",
    "list_top_k = []\n",
    "for i in range(similarity.shape[0]):\n",
    "    top_k_idx = np.argsort(similarity[i])[::-1][:top_k]\n",
    "    top_k_sim = similarity[i][top_k_idx]\n",
    "    list_top_k.append([(sim,zh_sentences[j]) for sim, j in zip(top_k_sim,top_k_idx)])\n",
    "\n",
    "reddit['top_1'] = [x[0][1] for x in list_top_k]\n",
    "reddit['top_1_sim'] = [x[0][0] for x in list_top_k]\n",
    "reddit['top_2'] = [x[1][1] for x in list_top_k]\n",
    "reddit['top_2_sim'] = [x[1][0] for x in list_top_k]\n",
    "reddit['top_3'] = [x[2][1] for x in list_top_k]\n",
    "reddit['top_3_sim'] = [x[2][0] for x in list_top_k]\n",
    "reddit['top_4'] = [x[3][1] for x in list_top_k]\n",
    "reddit['top_4_sim'] = [x[3][0] for x in list_top_k]\n",
    "reddit['top_5'] = [x[4][1] for x in list_top_k]\n",
    "reddit['top_5_sim'] = [x[4][0] for x in list_top_k]\n",
    "\n",
    "reddit.to_csv('../data/reddit_zhihu_matched_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv('../data/reddit_sample.csv')\n",
    "reddit.columns = [\"question\", \"detail\", \"answer\", \"topic\", \"answer_length\"]\n",
    "reddit_questions = reddit['question'].tolist()\n",
    "reddit_details = reddit['detail'].tolist()\n",
    "reddit_sentences = [str(x) + \" \" + str(y) for x, y in zip(reddit_questions, reddit_details)]\n",
    "\n",
    "\n",
    "reddit_emb = emb_model.encode(reddit_sentences)['dense_vecs']\n",
    "\n",
    "similarity = reddit_emb @ zh_emb.T\n",
    "\n",
    "top_k = 5\n",
    "list_top_k = []\n",
    "for i in range(similarity.shape[0]):\n",
    "    top_k_idx = np.argsort(similarity[i])[::-1][:top_k]\n",
    "    top_k_sim = similarity[i][top_k_idx]\n",
    "    list_top_k.append([(sim,zh_sentences[j]) for sim, j in zip(top_k_sim,top_k_idx)])\n",
    "\n",
    "reddit['top_1'] = [x[0][1] for x in list_top_k]\n",
    "reddit['top_1_sim'] = [x[0][0] for x in list_top_k]\n",
    "reddit['top_2'] = [x[1][1] for x in list_top_k]\n",
    "reddit['top_2_sim'] = [x[1][0] for x in list_top_k]\n",
    "reddit['top_3'] = [x[2][1] for x in list_top_k]\n",
    "reddit['top_3_sim'] = [x[2][0] for x in list_top_k]\n",
    "reddit['top_4'] = [x[3][1] for x in list_top_k]\n",
    "reddit['top_4_sim'] = [x[3][0] for x in list_top_k]\n",
    "reddit['top_5'] = [x[4][1] for x in list_top_k]\n",
    "reddit['top_5_sim'] = [x[4][0] for x in list_top_k]\n",
    "\n",
    "reddit.to_csv('../data/reddit_zhihu_matched_sample_wDetail.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM annotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv('../data/reddit_zhihu_matched_sample_wDetail.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ir_top5(question, candidates):\n",
    "        \n",
    "    client = OpenAI(api_key = os.environ['OPENAI_API_KEY'])\n",
    "    user_prompt = f\"# Question:\\n{question}\\n\\n # Candidates:\\nA. {candidates[0]}\\nB. {candidates[1]}\\nC. {candidates[2]}\\nD. {candidates[3]}\\nE. {candidates[4]}\\n\\n\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Given an English question, pick the most similar one from the list of 5 Chinese questions.\"\n",
    "                }\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "            \"name\": \"similar_question_response\",\n",
    "            \"strict\": True,\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                \"response\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The letter corresponding to the most similar question.\",\n",
    "                    \"enum\": [\n",
    "                    \"A\",\n",
    "                    \"B\",\n",
    "                    \"C\",\n",
    "                    \"D\",\n",
    "                    \"E\"\n",
    "                    ]\n",
    "                }\n",
    "                },\n",
    "                \"required\": [\n",
    "                \"response\"\n",
    "                ],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "            }\n",
    "        },\n",
    "        temperature=1,\n",
    "        max_completion_tokens=2048,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "        )\n",
    "    \n",
    "    return json.loads(response.choices[0].message.content)[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit[\"gpt_pick\"] = reddit.apply(lambda x: ir_top5(str(x['question'])+\"\\n\"+str(x['detail']), [x['top_1'], x['top_2'], x['top_3'], x['top_4'], x['top_5']]), axis=1)\n",
    "reddit[\"gpt_pick_question\"] = reddit.apply(lambda x: x[f\"top_{'_ABCDE'.index(x['gpt_pick'])}\"], axis=1)\n",
    "\n",
    "reddit.to_csv('../data/reddit_zhihu_matched_sample_wDetail.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_check(eng_q, chi_q):\n",
    "        \n",
    "    client = OpenAI(api_key = os.environ['OPENAI_API_KEY'])\n",
    "    user_prompt = f\"# English Question:\\n{eng_q}\\n\\n # Chinese Question:\\n{chi_q}\\n\\n\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Given an English question and a Chinese question, determine whether they are asking the same question.\"\n",
    "                }\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "            \"name\": \"similar_question_response\",\n",
    "            \"strict\": True,\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                \"response\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"Whether the English question and Chinese question are asking the same question.\",\n",
    "                }\n",
    "                },\n",
    "                \"required\": [\n",
    "                \"response\"\n",
    "                ],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "            }\n",
    "        },\n",
    "        temperature=1,\n",
    "        max_completion_tokens=2048,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "        )\n",
    "    \n",
    "    return json.loads(response.choices[0].message.content)[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit[\"gpt_sim\"] = reddit.apply(lambda x: sim_check(str(x['question'])+\"\\n\"+str(x['detail']), x['gpt_pick_question']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.to_csv('../data/reddit_zhihu_matched_sample_wDetail.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ir_top5_alt(question, candidates):\n",
    "        \n",
    "    client = OpenAI(api_key = os.environ['OPENAI_API_KEY'])\n",
    "    user_prompt = f\"# Question:\\n{question}\\n\\n # Candidates:\\nA. {candidates[0]}\\nB. {candidates[1]}\\nC. {candidates[2]}\\nD. {candidates[3]}\\nE. {candidates[4]}\\n\\n\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Given an English question and a list of 5 Chinese questions, pick the Chinese question that asks the same question as the English question. If none of the Chinese questions ask the same question, select 'F'.\"\n",
    "                }\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "            \"name\": \"similar_question_response\",\n",
    "            \"strict\": True,\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                \"response\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The letter corresponding to the most similar question. if none of the Chinese questions ask the same question, select 'F'.\",\n",
    "                    \"enum\": [\n",
    "                    \"A\",\n",
    "                    \"B\",\n",
    "                    \"C\",\n",
    "                    \"D\",\n",
    "                    \"E\",\n",
    "                    \"F\"\n",
    "                    ]\n",
    "                }\n",
    "                },\n",
    "                \"required\": [\n",
    "                \"response\"\n",
    "                ],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "            }\n",
    "        },\n",
    "        temperature=1,\n",
    "        max_completion_tokens=2048,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "        )\n",
    "    \n",
    "    return json.loads(response.choices[0].message.content)[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit[\"gpt_pick_alt\"] = reddit.apply(lambda x: ir_top5_alt(str(x['question'])+\"\\n\"+str(x['detail']), [x['top_1'], x['top_2'], x['top_3'], x['top_4'], x['top_5']]), axis=1)\n",
    "reddit.to_csv('../data/reddit_zhihu_matched_sample_wDetail.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expense calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv('../data/reddit_zhihu_matched_sample_wDetail.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(text, model=\"gpt-4o-mini\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Example usage:\n",
    "text = \"Hello, how are you?\"\n",
    "print(count_tokens(text))  # Output: Token count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_token_len = np.mean([count_tokens(str(x)) for x in reddit['question'].tolist()])\n",
    "detail_token_len = np.mean([count_tokens(str(x)) for x in reddit['detail'].tolist()])\n",
    "q1_token_len = np.mean([count_tokens(str(x)) for x in reddit['top_1'].tolist()])\n",
    "q2_token_len = np.mean([count_tokens(str(x)) for x in reddit['top_2'].tolist()])\n",
    "q3_token_len = np.mean([count_tokens(str(x)) for x in reddit['top_3'].tolist()])\n",
    "q4_token_len = np.mean([count_tokens(str(x)) for x in reddit['top_4'].tolist()])\n",
    "q5_token_len = np.mean([count_tokens(str(x)) for x in reddit['top_5'].tolist()])\n",
    "gpt_pick_question_token_len = np.mean([count_tokens(str(x)) for x in reddit['gpt_pick_question'].tolist()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1 token count:  640\n",
      "Method 2 token count:  352\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "ir_top_5_token = question_token_len + detail_token_len + q1_token_len + q2_token_len + q3_token_len + q4_token_len + q5_token_len\n",
    "sim_check_token = question_token_len + detail_token_len + gpt_pick_question_token_len\n",
    "\n",
    "method_1_token = ir_top_5_token + sim_check_token\n",
    "\n",
    "# method 2\n",
    "method_2_token = ir_top_5_token\n",
    "\n",
    "print(\"Method 1 token count: \", round(method_1_token))\n",
    "print(\"Method 2 token count: \", round(method_2_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1 cost: 4o - $46.06, 4o-mini - $2.76, effective outcome size: 9507, precision: 0.8\n",
      "Method 2 cost: 4o - $25.36, 4o-mini - $1.52, effective outcome size: 17285, precision: 0.61\n"
     ]
    }
   ],
   "source": [
    "# gpt-4o-mini: $0.150 / 1M tokens\n",
    "# gpt-4o:      $2.50 / 1M tokens\n",
    "price1_mini = 28809*method_1_token*(0.15/1e6)\n",
    "price1_4o = 28809*method_1_token*(2.5/1e6)\n",
    "price2_mini = 28809*method_2_token*(0.15/1e6)\n",
    "price2_4o = 28809*method_2_token*(2.5/1e6)\n",
    "\n",
    "print(f\"Method 1 cost: 4o - ${round(price1_4o, 2)}, 4o-mini - ${round(price1_mini, 2)}, effective outcome size: {round(28809*0.33)}, precision: 0.8\")\n",
    "print(f\"Method 2 cost: 4o - ${round(price2_4o, 2)}, 4o-mini - ${round(price2_mini, 2)}, effective outcome size: {round(28809*0.6)}, precision: 0.61\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
