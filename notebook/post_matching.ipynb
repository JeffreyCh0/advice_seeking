{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle   \n",
    "from openai import OpenAI\n",
    "import os\n",
    "import multiprocess as mp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/emb/zhihu_emb.pkl', 'rb') as f:\n",
    "    raw = pickle.load(f)\n",
    "    zh_sentences = raw[0]\n",
    "    zh_emb = raw[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf62a9ad56e4f079f0d5f2e850184e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "# emb_model = BGEM3FlagModel('BAAI/bge-m3',  \n",
    "#                        use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv('../data/reddit_post.csv')\n",
    "reddit = reddit[[\"message_id\", \"title\", \"message\"]]\n",
    "reddit.columns = [\"message_id\",\"question\", \"detail\"]\n",
    "reddit_questions = reddit['question'].tolist()\n",
    "reddit_details = reddit['detail'].tolist()\n",
    "reddit_sentences = [str(x) + \" \" + str(y) for x, y in zip(reddit_questions, reddit_details)]\n",
    "\n",
    "\n",
    "# reddit_emb = emb_model.encode(reddit_sentences)['dense_vecs']\n",
    "\n",
    "# with open('../data/emb/reddit_emb.pkl', 'wb') as f:\n",
    "#     pickle.dump([reddit_sentences, reddit_emb], f)\n",
    "\n",
    "with open('../data/emb/reddit_emb.pkl', 'rb') as f:\n",
    "    raw = pickle.load(f)\n",
    "    reddit_sentences = raw[0]\n",
    "    reddit_emb = raw[1]\n",
    "\n",
    "similarity = reddit_emb @ zh_emb.T\n",
    "\n",
    "with open('../data/emb/similarity.pkl', 'wb') as f:\n",
    "    pickle.dump(similarity, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ir_top5(question, candidates):\n",
    "        \n",
    "    client = OpenAI(api_key = os.environ['OPENAI_API_KEY'])\n",
    "    user_prompt = f\"# Question:\\n{question}\\n\\n # Candidates:\\nA. {candidates[0]}\\nB. {candidates[1]}\\nC. {candidates[2]}\\nD. {candidates[3]}\\nE. {candidates[4]}\\n\\n\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Given an English question, pick the most similar one from the list of 5 Chinese questions.\"\n",
    "                }\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "            \"name\": \"similar_question_response\",\n",
    "            \"strict\": True,\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                \"response\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The letter corresponding to the most similar question.\",\n",
    "                    \"enum\": [\n",
    "                    \"A\",\n",
    "                    \"B\",\n",
    "                    \"C\",\n",
    "                    \"D\",\n",
    "                    \"E\"\n",
    "                    ]\n",
    "                }\n",
    "                },\n",
    "                \"required\": [\n",
    "                \"response\"\n",
    "                ],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "            }\n",
    "        },\n",
    "        temperature=1,\n",
    "        max_completion_tokens=2048,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "        )\n",
    "    \n",
    "    return json.loads(response.choices[0].message.content)[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28809 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "reddit = pd.read_csv('../data/reddit_post.csv')\n",
    "reddit = reddit[[\"message_id\", \"title\", \"message\"]]\n",
    "reddit.columns = [\"message_id\",\"question\", \"detail\"]\n",
    "\n",
    "with open('../data/emb/similarity.pkl', 'rb') as f:\n",
    "    similarity = pickle.load(f)\n",
    "    similarity = similarity.T\n",
    "\n",
    "with open('../data/emb/zhihu_emb.pkl', 'rb') as f:\n",
    "    raw = pickle.load(f)\n",
    "    zh_sentences = raw[0]\n",
    "    zh_emb = raw[1]\n",
    "\n",
    "def process_row(question_detail, choices):\n",
    "    \"\"\"Function to process each row\"\"\"\n",
    "    gpt_pick = ir_top5(question_detail, choices)\n",
    "    gpt_pick_question = choices['ABCDE'.index(gpt_pick)]\n",
    "    return gpt_pick, gpt_pick_question\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "results = []\n",
    "top_k = 5\n",
    "list_top_k = []\n",
    "for i, row in tqdm(enumerate(reddit.itertuples(index=False)), total = reddit.shape[0]): #tqdm(enumerate(reddit.iterrows())):\n",
    "    top_k_idx = np.argsort(similarity[i])[::-1][:top_k]\n",
    "    top_k_sim = similarity[i][top_k_idx]\n",
    "    list_top_k.append([(sim,zh_sentences[j]) for sim, j in zip(top_k_sim,top_k_idx)])\n",
    "    \n",
    "    question_detail = str(row.question) + \"\\n\" + str(row.detail)\n",
    "    choices = [zh_sentences[j] for j in top_k_idx]\n",
    "    gpt_pick, gpt_pick_question = process_row(question_detail, choices)\n",
    "    results.append((gpt_pick, gpt_pick_question))\n",
    "\n",
    "    # without replacement\n",
    "    zh_sentences = np.delete(zh_sentences, top_k_idx)\n",
    "    zh_emb = np.delete(zh_emb, top_k_idx, axis=0)\n",
    "    similarity = np.delete(similarity, top_k_idx, axis=1)\n",
    "\n",
    "reddit['top_1'] = [x[0][1] for x in list_top_k]\n",
    "reddit['top_1_sim'] = [x[0][0] for x in list_top_k]\n",
    "reddit['top_2'] = [x[1][1] for x in list_top_k]\n",
    "reddit['top_2_sim'] = [x[1][0] for x in list_top_k]\n",
    "reddit['top_3'] = [x[2][1] for x in list_top_k]\n",
    "reddit['top_3_sim'] = [x[2][0] for x in list_top_k]\n",
    "reddit['top_4'] = [x[3][1] for x in list_top_k]\n",
    "reddit['top_4_sim'] = [x[3][0] for x in list_top_k]\n",
    "reddit['top_5'] = [x[4][1] for x in list_top_k]\n",
    "reddit['top_5_sim'] = [x[4][0] for x in list_top_k]\n",
    "\n",
    "reddit[\"gpt_pick\"], reddit[\"gpt_pick_question\"] = zip(*results)\n",
    "\n",
    "\n",
    "\n",
    "reddit.to_csv('../data/matched_gpt_4o_mini.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_check(eng_q, chi_q):\n",
    "    api_key = os.environ['OPENAI_API_KEY']\n",
    "    client = OpenAI(api_key = api_key)\n",
    "    user_prompt = f\"# English Question:\\n{eng_q}\\n\\n # Chinese Question:\\n{chi_q}\\n\\n\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Given an English question and a Chinese question, determine whether they are asking the same question.\"\n",
    "                }\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "            \"name\": \"similar_question_response\",\n",
    "            \"strict\": True,\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                \"response\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"Whether the English question and Chinese question are asking the same question.\",\n",
    "                }\n",
    "                },\n",
    "                \"required\": [\n",
    "                \"response\"\n",
    "                ],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "            }\n",
    "        },\n",
    "        temperature=1,\n",
    "        max_completion_tokens=2048,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "        )\n",
    "    \n",
    "    return json.loads(response.choices[0].message.content)[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28809/28809 [08:21<00:00, 57.42it/s] \n"
     ]
    }
   ],
   "source": [
    "# reddit[\"gpt_sim\"] = reddit.apply(lambda x: sim_check(str(x['question'])+\"\\n\"+str(x['detail']), x['gpt_pick_question']), axis=1)\n",
    "\n",
    "reddit = pd.read_csv('../data/matched_gpt_4o_mini.csv')\n",
    "\n",
    "def sim_check_wrapper(args):\n",
    "    \"\"\" Wrapper function for parallel processing \"\"\"\n",
    "    row, sim_check_func = args\n",
    "    return sim_check_func(str(row['question']) + \"\\n\" + str(row['detail']), row['gpt_pick_question'])\n",
    "\n",
    "def parallel_apply(df, func, num_workers=4):\n",
    "    \"\"\" Parallel apply using multiprocessing \"\"\"\n",
    "    with mp.Pool(num_workers) as pool:\n",
    "        results = list(tqdm(pool.imap(sim_check_wrapper, [(row, func) for _, row in df.iterrows()]), total=len(df)))\n",
    "    return results\n",
    "\n",
    "# Assuming reddit is your DataFrame and sim_check is your function\n",
    "reddit[\"gpt_sim\"] = parallel_apply(reddit, sim_check, num_workers=mp.cpu_count())\n",
    "reddit.to_csv('../data/matched_gpt_4o_mini.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ratio: 0.014623193328050114\n"
     ]
    }
   ],
   "source": [
    "# reddit = pd.read_csv('../data/r_matched_gpt_4o_mini.csv')\n",
    "\n",
    "count_false = reddit[reddit['gpt_sim'] == False].shape[0]\n",
    "count_true = reddit[reddit['gpt_sim'] == True].shape[0]\n",
    "true_ratio = count_true / (count_false + count_true)\n",
    "print(f\"True ratio: {true_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2325, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit[reddit['gpt_sim'] == True].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2325, 13)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit[reddit['gpt_sim'] == True].drop_duplicates(subset=['zh_question']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit[reddit['gpt_sim'] == True].to_csv('../data/r_filtered_gpt_4o_mini.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jch0/.conda/envs/jch0/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"wangrui6/Zhihu-KOL\")\n",
    "\n",
    "data_dict = {}\n",
    "data_dict['question'] = ds['train']['INSTRUCTION']\n",
    "data_dict['answer'] = ds['train']['RESPONSE']\n",
    "for key in json.loads(ds['train']['METADATA'][0]).keys():\n",
    "    data_dict[key] = [json.loads(x)[key] for x in ds['train']['METADATA']]\n",
    "df_train = pd.DataFrame(data_dict)\n",
    "\n",
    "def clean_upvotes(upvote_str):\n",
    "    upvote_str = upvote_str.replace('赞同', '').strip()\n",
    "    if len(upvote_str) > 0:\n",
    "        if \"万\" in upvote_str:\n",
    "            upvote_str = upvote_str.replace('万', '').strip()\n",
    "            return int(float(upvote_str) * 10000)\n",
    "        else:\n",
    "            return int(upvote_str)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "df_train['upvotes'] = df_train['upvotes'].apply(clean_upvotes)\n",
    "df_train['answer_id'] = df_train['answer_id'].astype('int64')\n",
    "df_train['question_id'] = df_train['question_id'].astype('int64')\n",
    "\n",
    "# df_train = df_train[df_train['url'].notna()] # use only url is not NaN\n",
    "print(df_train.shape[0])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2325\n",
      "2325\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zh_question_id</th>\n",
       "      <th>zh_answer_id</th>\n",
       "      <th>zh_answer</th>\n",
       "      <th>zh_upvotes</th>\n",
       "      <th>zh_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>352160616</td>\n",
       "      <td>911748450</td>\n",
       "      <td>2022.9月20日更新 其实我在2022年一月已经彻底放弃虾皮哈哈哈哈，老子不玩啦。———...</td>\n",
       "      <td>231</td>\n",
       "      <td>https://www.zhihu.com/question/352160616/answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>570091915</td>\n",
       "      <td>2784599789</td>\n",
       "      <td>1，科学饮食，可多摄入富含优质蛋白的食物，例如多吃肉类、鸡蛋以及豆制品等，还需要多吃新鲜的蔬...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.zhihu.com/question/570091915/answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20948066</td>\n",
       "      <td>701579176</td>\n",
       "      <td>如今伴随着耳机大厂商都各自推出带有主动降噪耳机，一时间降噪耳机似乎变成了一种流行趋势。一副主...</td>\n",
       "      <td>334</td>\n",
       "      <td>https://www.zhihu.com/question/20948066/answer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>19849704</td>\n",
       "      <td>232291540</td>\n",
       "      <td>1 12年初到北京，在某小型（却土豪）香港驻京办电影公司工作了三年，期间写了不少东西，却浑浑...</td>\n",
       "      <td>728</td>\n",
       "      <td>https://www.zhihu.com/question/19849704/answer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>373924698</td>\n",
       "      <td>1056437024</td>\n",
       "      <td>现在疫情比较严重，更多的时候都是出不了门，还是有一些好玩的，可以在家或在学校做的线上公益。 ...</td>\n",
       "      <td>7</td>\n",
       "      <td>https://www.zhihu.com/question/373924698/answe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    zh_question_id  zh_answer_id  \\\n",
       "4        352160616     911748450   \n",
       "5        570091915    2784599789   \n",
       "11        20948066     701579176   \n",
       "27        19849704     232291540   \n",
       "34       373924698    1056437024   \n",
       "\n",
       "                                            zh_answer  zh_upvotes  \\\n",
       "4   2022.9月20日更新 其实我在2022年一月已经彻底放弃虾皮哈哈哈哈，老子不玩啦。———...         231   \n",
       "5   1，科学饮食，可多摄入富含优质蛋白的食物，例如多吃肉类、鸡蛋以及豆制品等，还需要多吃新鲜的蔬...           0   \n",
       "11  如今伴随着耳机大厂商都各自推出带有主动降噪耳机，一时间降噪耳机似乎变成了一种流行趋势。一副主...         334   \n",
       "27  1 12年初到北京，在某小型（却土豪）香港驻京办电影公司工作了三年，期间写了不少东西，却浑浑...         728   \n",
       "34  现在疫情比较严重，更多的时候都是出不了门，还是有一些好玩的，可以在家或在学校做的线上公益。 ...           7   \n",
       "\n",
       "                                               zh_url  \n",
       "4   https://www.zhihu.com/question/352160616/answe...  \n",
       "5   https://www.zhihu.com/question/570091915/answe...  \n",
       "11  https://www.zhihu.com/question/20948066/answer...  \n",
       "27  https://www.zhihu.com/question/19849704/answer...  \n",
       "34  https://www.zhihu.com/question/373924698/answe...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtered = reddit[reddit['gpt_sim'] == True]\n",
    "filtered = pd.read_csv('../data/r_filtered_gpt_4o_mini.csv')\n",
    "\n",
    "# merge with train data, left on gpt_pick_question and right on question\n",
    "# leave only the rows that has most upvotes\n",
    "\n",
    "merged = pd.merge(filtered, df_train, left_on='zh_question', right_on='question_id', how='left')\n",
    "merged = merged[merged['upvotes'] == merged.groupby('zh_question')['upvotes'].transform('max')]\n",
    "merged = merged.drop_duplicates(subset=['zh_question'])\n",
    "merged = merged[['zh_question', 'answer_id', 'answer', 'upvotes', 'url']]\n",
    "merged.columns = ['zh_question_id', 'zh_answer_id', 'zh_answer', 'zh_upvotes', 'zh_url']\n",
    "merged['zh_upvotes'] = merged['zh_upvotes'].astype('int64')\n",
    "merged['zh_answer_id'] = merged['zh_answer_id'].astype('int64')\n",
    "# merged = merged[merged['zhihu_upvotes']>0]\n",
    "print(merged.shape[0])\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('../data/zh_q_a.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5397, 8)\n",
      "(5191, 8)\n",
      "(1848, 8)\n"
     ]
    }
   ],
   "source": [
    "# deduplication\n",
    "import pandas as pd\n",
    "\n",
    "merged = pd.read_csv('../data/filtered_gpt_4o_mini.csv')\n",
    "print(merged.shape)\n",
    "merged = merged.drop_duplicates(subset=['reddit_message_id'])\n",
    "print(merged.shape)\n",
    "merged = merged.drop_duplicates(subset=['zhihu_question_id'])\n",
    "print(merged.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pick random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zh_question</th>\n",
       "      <th>gpt_pick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2165</th>\n",
       "      <td>512155685</td>\n",
       "      <td>dv4lw1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>553799534</td>\n",
       "      <td>d8bota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>30235984</td>\n",
       "      <td>z9r94s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>323536094</td>\n",
       "      <td>wf8kub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>583086250</td>\n",
       "      <td>dz7460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      zh_question gpt_pick\n",
       "2165    512155685   dv4lw1\n",
       "563     553799534   d8bota\n",
       "791      30235984   z9r94s\n",
       "1330    323536094   wf8kub\n",
       "570     583086250   dz7460"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filtered = pd.read_csv('../data/r_filtered_gpt_4o_mini.csv')\n",
    "filtered = filtered[[\"zh_question\", \"gpt_pick\"]]\n",
    "filtered = filtered.sample(100, random_state=0)\n",
    "filtered.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['zh_question_id', 'zh_answer_id', 'zh_answer', 'zh_upvotes', 'zh_url'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train[[\"\"]]\n",
    "filt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jch0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
