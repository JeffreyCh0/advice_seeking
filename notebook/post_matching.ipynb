{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle   \n",
    "from openai import OpenAI\n",
    "import os\n",
    "import multiprocess as mp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/emb/zhihu_emb.pkl', 'rb') as f:\n",
    "    raw = pickle.load(f)\n",
    "    zh_sentences = raw[0]\n",
    "    zh_emb = raw[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf62a9ad56e4f079f0d5f2e850184e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "# emb_model = BGEM3FlagModel('BAAI/bge-m3',  \n",
    "#                        use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv('../data/reddit_post.csv')\n",
    "reddit = reddit[[\"message_id\", \"title\", \"message\"]]\n",
    "reddit.columns = [\"message_id\",\"question\", \"detail\"]\n",
    "reddit_questions = reddit['question'].tolist()\n",
    "reddit_details = reddit['detail'].tolist()\n",
    "reddit_sentences = [str(x) + \" \" + str(y) for x, y in zip(reddit_questions, reddit_details)]\n",
    "\n",
    "\n",
    "# reddit_emb = emb_model.encode(reddit_sentences)['dense_vecs']\n",
    "\n",
    "# with open('../data/emb/reddit_emb.pkl', 'wb') as f:\n",
    "#     pickle.dump([reddit_sentences, reddit_emb], f)\n",
    "\n",
    "with open('../data/emb/reddit_emb.pkl', 'rb') as f:\n",
    "    raw = pickle.load(f)\n",
    "    reddit_sentences = raw[0]\n",
    "    reddit_emb = raw[1]\n",
    "\n",
    "similarity = reddit_emb @ zh_emb.T\n",
    "\n",
    "with open('../data/emb/similarity.pkl', 'wb') as f:\n",
    "    pickle.dump(similarity, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ir_top5(question, candidates):\n",
    "        \n",
    "    client = OpenAI(api_key = os.environ['OPENAI_API_KEY'])\n",
    "    user_prompt = f\"# Question:\\n{question}\\n\\n # Candidates:\\nA. {candidates[0]}\\nB. {candidates[1]}\\nC. {candidates[2]}\\nD. {candidates[3]}\\nE. {candidates[4]}\\n\\n\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Given an English question, pick the most similar one from the list of 5 Chinese questions.\"\n",
    "                }\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "            \"name\": \"similar_question_response\",\n",
    "            \"strict\": True,\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                \"response\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The letter corresponding to the most similar question.\",\n",
    "                    \"enum\": [\n",
    "                    \"A\",\n",
    "                    \"B\",\n",
    "                    \"C\",\n",
    "                    \"D\",\n",
    "                    \"E\"\n",
    "                    ]\n",
    "                }\n",
    "                },\n",
    "                \"required\": [\n",
    "                \"response\"\n",
    "                ],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "            }\n",
    "        },\n",
    "        temperature=1,\n",
    "        max_completion_tokens=2048,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "        )\n",
    "    \n",
    "    return json.loads(response.choices[0].message.content)[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28809 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "reddit = pd.read_csv('../data/reddit_post.csv')\n",
    "reddit = reddit[[\"message_id\", \"title\", \"message\"]]\n",
    "reddit.columns = [\"message_id\",\"question\", \"detail\"]\n",
    "\n",
    "with open('../data/emb/similarity.pkl', 'rb') as f:\n",
    "    similarity = pickle.load(f)\n",
    "    similarity = similarity.T\n",
    "\n",
    "with open('../data/emb/zhihu_emb.pkl', 'rb') as f:\n",
    "    raw = pickle.load(f)\n",
    "    zh_sentences = raw[0]\n",
    "    zh_emb = raw[1]\n",
    "\n",
    "def process_row(question_detail, choices):\n",
    "    \"\"\"Function to process each row\"\"\"\n",
    "    gpt_pick = ir_top5(question_detail, choices)\n",
    "    gpt_pick_question = choices['ABCDE'.index(gpt_pick)]\n",
    "    return gpt_pick, gpt_pick_question\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "results = []\n",
    "top_k = 5\n",
    "list_top_k = []\n",
    "for i, row in tqdm(enumerate(reddit.itertuples(index=False)), total = reddit.shape[0]): #tqdm(enumerate(reddit.iterrows())):\n",
    "    top_k_idx = np.argsort(similarity[i])[::-1][:top_k]\n",
    "    top_k_sim = similarity[i][top_k_idx]\n",
    "    list_top_k.append([(sim,zh_sentences[j]) for sim, j in zip(top_k_sim,top_k_idx)])\n",
    "    \n",
    "    question_detail = str(row.question) + \"\\n\" + str(row.detail)\n",
    "    choices = [zh_sentences[j] for j in top_k_idx]\n",
    "    gpt_pick, gpt_pick_question = process_row(question_detail, choices)\n",
    "    results.append((gpt_pick, gpt_pick_question))\n",
    "\n",
    "    # without replacement\n",
    "    zh_sentences = np.delete(zh_sentences, top_k_idx)\n",
    "    zh_emb = np.delete(zh_emb, top_k_idx, axis=0)\n",
    "    similarity = np.delete(similarity, top_k_idx, axis=1)\n",
    "\n",
    "reddit['top_1'] = [x[0][1] for x in list_top_k]\n",
    "reddit['top_1_sim'] = [x[0][0] for x in list_top_k]\n",
    "reddit['top_2'] = [x[1][1] for x in list_top_k]\n",
    "reddit['top_2_sim'] = [x[1][0] for x in list_top_k]\n",
    "reddit['top_3'] = [x[2][1] for x in list_top_k]\n",
    "reddit['top_3_sim'] = [x[2][0] for x in list_top_k]\n",
    "reddit['top_4'] = [x[3][1] for x in list_top_k]\n",
    "reddit['top_4_sim'] = [x[3][0] for x in list_top_k]\n",
    "reddit['top_5'] = [x[4][1] for x in list_top_k]\n",
    "reddit['top_5_sim'] = [x[4][0] for x in list_top_k]\n",
    "\n",
    "reddit[\"gpt_pick\"], reddit[\"gpt_pick_question\"] = zip(*results)\n",
    "\n",
    "\n",
    "\n",
    "reddit.to_csv('../data/matched_gpt_4o_mini.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_check(eng_q, chi_q):\n",
    "    api_key = os.environ['OPENAI_API_KEY']\n",
    "    client = OpenAI(api_key = api_key)\n",
    "    user_prompt = f\"# English Question:\\n{eng_q}\\n\\n # Chinese Question:\\n{chi_q}\\n\\n\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Given an English question and a Chinese question, determine whether they are asking the same question.\"\n",
    "                }\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "            \"name\": \"similar_question_response\",\n",
    "            \"strict\": True,\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                \"response\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"Whether the English question and Chinese question are asking the same question.\",\n",
    "                }\n",
    "                },\n",
    "                \"required\": [\n",
    "                \"response\"\n",
    "                ],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "            }\n",
    "        },\n",
    "        temperature=1,\n",
    "        max_completion_tokens=2048,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "        )\n",
    "    \n",
    "    return json.loads(response.choices[0].message.content)[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28809/28809 [08:21<00:00, 57.42it/s] \n"
     ]
    }
   ],
   "source": [
    "# reddit[\"gpt_sim\"] = reddit.apply(lambda x: sim_check(str(x['question'])+\"\\n\"+str(x['detail']), x['gpt_pick_question']), axis=1)\n",
    "\n",
    "reddit = pd.read_csv('../data/matched_gpt_4o_mini.csv')\n",
    "\n",
    "def sim_check_wrapper(args):\n",
    "    \"\"\" Wrapper function for parallel processing \"\"\"\n",
    "    row, sim_check_func = args\n",
    "    return sim_check_func(str(row['question']) + \"\\n\" + str(row['detail']), row['gpt_pick_question'])\n",
    "\n",
    "def parallel_apply(df, func, num_workers=4):\n",
    "    \"\"\" Parallel apply using multiprocessing \"\"\"\n",
    "    with mp.Pool(num_workers) as pool:\n",
    "        results = list(tqdm(pool.imap(sim_check_wrapper, [(row, func) for _, row in df.iterrows()]), total=len(df)))\n",
    "    return results\n",
    "\n",
    "# Assuming reddit is your DataFrame and sim_check is your function\n",
    "reddit[\"gpt_sim\"] = parallel_apply(reddit, sim_check, num_workers=mp.cpu_count())\n",
    "reddit.to_csv('../data/matched_gpt_4o_mini.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ratio: 0.04148009302648478\n"
     ]
    }
   ],
   "source": [
    "count_false = reddit[reddit['gpt_sim'] == False].shape[0]\n",
    "count_true = reddit[reddit['gpt_sim'] == True].shape[0]\n",
    "true_ratio = count_true / (count_false + count_true)\n",
    "print(f\"True ratio: {true_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5879, 16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit[reddit['gpt_sim'] == True].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1195, 16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit[reddit['gpt_sim'] == True].drop_duplicates(subset=['message_id']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit[reddit['gpt_sim'] == True].to_csv('../data/filtered_gpt_4o_mini.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>url</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>answer_creation_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>怎么说服男朋友买烤箱？</td>\n",
       "      <td>emmmmm，首先想说的是，我买厨房用品一般是不用「说服」的，只是在厨房堆的满满当当的情况下...</td>\n",
       "      <td>357137111</td>\n",
       "      <td>914332816</td>\n",
       "      <td>https://www.zhihu.com/question/357137111/answe...</td>\n",
       "      <td>15</td>\n",
       "      <td>2019-11-28T12:01:22.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>航天从业者是如何看待电视剧《你是我的荣耀》的？</td>\n",
       "      <td>难得有个关于航天的剧，职场情节悬不悬浮，航天设定和细节走不走心？带着放大镜看了前18集，...</td>\n",
       "      <td>475169837</td>\n",
       "      <td>2053313113</td>\n",
       "      <td>https://www.zhihu.com/question/475169837/answe...</td>\n",
       "      <td>4432</td>\n",
       "      <td>2021-08-11T07:26:08.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>如何看待PayPal正式进入中国？</td>\n",
       "      <td>PayPal不仅是美国支付巨头，也是国际支付巨头，目前已开拓全球200多个市场，美国以外的市...</td>\n",
       "      <td>348551037</td>\n",
       "      <td>866103409</td>\n",
       "      <td>https://www.zhihu.com/question/348551037/answe...</td>\n",
       "      <td>127</td>\n",
       "      <td>2019-10-22T09:11:15.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>中金公司交易员月薪八万五是如何做到的？</td>\n",
       "      <td>1、首先，考虑到这位交易员的工作经验，月薪八万五的表述是不正确的：其实是一年的全部薪酬除以1...</td>\n",
       "      <td>545938899</td>\n",
       "      <td>2602363788</td>\n",
       "      <td>https://www.zhihu.com/question/545938899/answe...</td>\n",
       "      <td>450</td>\n",
       "      <td>2022-07-31T13:29:04.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>摇滚乐（金属）给你们带来了什么？</td>\n",
       "      <td>ㄟ( ▔, ▔ )ㄏ哪里带来了什么东西啊，除了找到热爱的东西，也失去了很多。听重型现场像疯子...</td>\n",
       "      <td>361437216</td>\n",
       "      <td>1073541478</td>\n",
       "      <td>https://www.zhihu.com/question/361437216/answe...</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-03-12T05:49:28.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  question                                             answer  \\\n",
       "0              怎么说服男朋友买烤箱？  emmmmm，首先想说的是，我买厨房用品一般是不用「说服」的，只是在厨房堆的满满当当的情况下...   \n",
       "1  航天从业者是如何看待电视剧《你是我的荣耀》的？    难得有个关于航天的剧，职场情节悬不悬浮，航天设定和细节走不走心？带着放大镜看了前18集，...   \n",
       "2        如何看待PayPal正式进入中国？  PayPal不仅是美国支付巨头，也是国际支付巨头，目前已开拓全球200多个市场，美国以外的市...   \n",
       "3      中金公司交易员月薪八万五是如何做到的？  1、首先，考虑到这位交易员的工作经验，月薪八万五的表述是不正确的：其实是一年的全部薪酬除以1...   \n",
       "4         摇滚乐（金属）给你们带来了什么？  ㄟ( ▔, ▔ )ㄏ哪里带来了什么东西啊，除了找到热爱的东西，也失去了很多。听重型现场像疯子...   \n",
       "\n",
       "   question_id   answer_id                                                url  \\\n",
       "0    357137111   914332816  https://www.zhihu.com/question/357137111/answe...   \n",
       "1    475169837  2053313113  https://www.zhihu.com/question/475169837/answe...   \n",
       "2    348551037   866103409  https://www.zhihu.com/question/348551037/answe...   \n",
       "3    545938899  2602363788  https://www.zhihu.com/question/545938899/answe...   \n",
       "4    361437216  1073541478  https://www.zhihu.com/question/361437216/answe...   \n",
       "\n",
       "   upvotes      answer_creation_time  \n",
       "0       15  2019-11-28T12:01:22.000Z  \n",
       "1     4432  2021-08-11T07:26:08.000Z  \n",
       "2      127  2019-10-22T09:11:15.000Z  \n",
       "3      450  2022-07-31T13:29:04.000Z  \n",
       "4        5  2020-03-12T05:49:28.000Z  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "ds = load_dataset(\"wangrui6/Zhihu-KOL\")\n",
    "\n",
    "data_dict = {}\n",
    "data_dict['question'] = ds['train']['INSTRUCTION']\n",
    "data_dict['answer'] = ds['train']['RESPONSE']\n",
    "for key in json.loads(ds['train']['METADATA'][0]).keys():\n",
    "    data_dict[key] = [json.loads(x)[key] for x in ds['train']['METADATA']]\n",
    "df_train = pd.DataFrame(data_dict)\n",
    "\n",
    "def clean_upvotes(upvote_str):\n",
    "    upvote_str = upvote_str.replace('赞同', '').strip()\n",
    "    if len(upvote_str) > 0:\n",
    "        if \"万\" in upvote_str:\n",
    "            upvote_str = upvote_str.replace('万', '').strip()\n",
    "            return int(float(upvote_str) * 10000)\n",
    "        else:\n",
    "            return int(upvote_str)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "df_train['upvotes'] = df_train['upvotes'].apply(clean_upvotes)\n",
    "df_train['answer_id'] = df_train['answer_id'].astype('int64')\n",
    "df_train['question_id'] = df_train['question_id'].astype('int64')\n",
    "\n",
    "df_train = df_train[df_train['url'].notna()] # use only url is not NaN\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reddit_message_id</th>\n",
       "      <th>reddit_question</th>\n",
       "      <th>reddit_detail</th>\n",
       "      <th>zhihu_question_id</th>\n",
       "      <th>zhihu_question</th>\n",
       "      <th>zhihu_answer</th>\n",
       "      <th>zhihu_upvotes</th>\n",
       "      <th>zhihu_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bkclcg</td>\n",
       "      <td>Should people not comment on what strangers ar...</td>\n",
       "      <td>I was sitting in the break room eating a SALAD...</td>\n",
       "      <td>505629130.0</td>\n",
       "      <td>跟同事一起吃饭，她爱吃米饭，我爱吃馒头几乎每次她都说最讨厌吃馒头了，我总觉得不太舒服，是我太...</td>\n",
       "      <td>不是太敏感，是你有病。 一，对方每次都说，你不反抗。懦弱讨好型人格。 二，对方每次说，你还和...</td>\n",
       "      <td>3773</td>\n",
       "      <td>https://www.zhihu.com/question/505629130/answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bkqbrm</td>\n",
       "      <td>Nothing makes me happy anymore and I want to k...</td>\n",
       "      <td>It's not like I have anything to complain abou...</td>\n",
       "      <td>461771809.0</td>\n",
       "      <td>目前大一，就感觉生活很无趣，有想离开这个世界的想法，很烦 我该怎么办？</td>\n",
       "      <td>无非生活不如意，经济不独立。想改变又没有勇气，也没有能力。 解决的办法有，因为我当时也和你一...</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.zhihu.com/question/461771809/answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bt7lhc</td>\n",
       "      <td>My parents won't let me go to a psychiatrist.</td>\n",
       "      <td>I am 16 I used one of the video call psychiatr...</td>\n",
       "      <td>311241635.0</td>\n",
       "      <td>父母不同意带我去做心理咨询怎么办？</td>\n",
       "      <td>分两部分答。 1，你需要帮助，如何自助。 2，心理咨询费有多高，为什么高。 很遗憾的看到，你...</td>\n",
       "      <td>26</td>\n",
       "      <td>https://www.zhihu.com/question/311241635/answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>br7xd2</td>\n",
       "      <td>My job is draining me</td>\n",
       "      <td>So i got my first job this year. I work at one...</td>\n",
       "      <td>56322737.0</td>\n",
       "      <td>工作太累已经影响了身体健康，该不该辞职？</td>\n",
       "      <td>我之前工作的CATL，太累，一个星期100h的工作量，简直是亏本买卖，入职1个月就辞职了。 ...</td>\n",
       "      <td>204</td>\n",
       "      <td>https://www.zhihu.com/question/56322737/answer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>bnisgy</td>\n",
       "      <td>My depression and my Body Dysmorphia has pulle...</td>\n",
       "      <td>So many times in my life, I have become a shut...</td>\n",
       "      <td>351617245.0</td>\n",
       "      <td>性格内向又有社交障碍的我真的过不下去了，该怎么办?</td>\n",
       "      <td>一、你需要的，是克服内心的恐惧，而不是改正自己的“毛病”。你没有任何毛病，就算有，大多数人也...</td>\n",
       "      <td>3084</td>\n",
       "      <td>https://www.zhihu.com/question/351617245/answe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reddit_message_id                                    reddit_question  \\\n",
       "1             bkclcg  Should people not comment on what strangers ar...   \n",
       "14            bkqbrm  Nothing makes me happy anymore and I want to k...   \n",
       "19            bt7lhc      My parents won't let me go to a psychiatrist.   \n",
       "22            br7xd2                              My job is draining me   \n",
       "44            bnisgy  My depression and my Body Dysmorphia has pulle...   \n",
       "\n",
       "                                        reddit_detail  zhihu_question_id  \\\n",
       "1   I was sitting in the break room eating a SALAD...        505629130.0   \n",
       "14  It's not like I have anything to complain abou...        461771809.0   \n",
       "19  I am 16 I used one of the video call psychiatr...        311241635.0   \n",
       "22  So i got my first job this year. I work at one...         56322737.0   \n",
       "44  So many times in my life, I have become a shut...        351617245.0   \n",
       "\n",
       "                                       zhihu_question  \\\n",
       "1   跟同事一起吃饭，她爱吃米饭，我爱吃馒头几乎每次她都说最讨厌吃馒头了，我总觉得不太舒服，是我太...   \n",
       "14                目前大一，就感觉生活很无趣，有想离开这个世界的想法，很烦 我该怎么办？   \n",
       "19                                  父母不同意带我去做心理咨询怎么办？   \n",
       "22                               工作太累已经影响了身体健康，该不该辞职？   \n",
       "44                          性格内向又有社交障碍的我真的过不下去了，该怎么办?   \n",
       "\n",
       "                                         zhihu_answer  zhihu_upvotes  \\\n",
       "1   不是太敏感，是你有病。 一，对方每次都说，你不反抗。懦弱讨好型人格。 二，对方每次说，你还和...           3773   \n",
       "14  无非生活不如意，经济不独立。想改变又没有勇气，也没有能力。 解决的办法有，因为我当时也和你一...              3   \n",
       "19  分两部分答。 1，你需要帮助，如何自助。 2，心理咨询费有多高，为什么高。 很遗憾的看到，你...             26   \n",
       "22  我之前工作的CATL，太累，一个星期100h的工作量，简直是亏本买卖，入职1个月就辞职了。 ...            204   \n",
       "44  一、你需要的，是克服内心的恐惧，而不是改正自己的“毛病”。你没有任何毛病，就算有，大多数人也...           3084   \n",
       "\n",
       "                                            zhihu_url  \n",
       "1   https://www.zhihu.com/question/505629130/answe...  \n",
       "14  https://www.zhihu.com/question/461771809/answe...  \n",
       "19  https://www.zhihu.com/question/311241635/answe...  \n",
       "22  https://www.zhihu.com/question/56322737/answer...  \n",
       "44  https://www.zhihu.com/question/351617245/answe...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered = reddit[reddit['gpt_sim'] == True]\n",
    "\n",
    "filtered.head()\n",
    "\n",
    "# merge with train data, left on gpt_pick_question and right on question\n",
    "# leave only the rows that has most upvotes\n",
    "\n",
    "merged = pd.merge(filtered, df_train, left_on='gpt_pick_question', right_on='question', how='left')\n",
    "merged = merged[merged['upvotes'] == merged.groupby('message_id')['upvotes'].transform('max')]\n",
    "merged = merged[['message_id', 'question_x', 'detail', 'question_id', 'gpt_pick_question', 'answer', 'upvotes', 'url']]\n",
    "merged.columns = ['reddit_message_id', 'reddit_question', 'reddit_detail', 'zhihu_question_id', 'zhihu_question', 'zhihu_answer', 'zhihu_upvotes', 'zhihu_url']\n",
    "merged['zhihu_upvotes'] = merged['zhihu_upvotes'].astype('int64')\n",
    "merged = merged[merged['zhihu_upvotes']>0]\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('../data/filtered_gpt_4o_mini.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zhihu_question</th>\n",
       "      <th>zhihu_answer</th>\n",
       "      <th>zhihu_upvotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36606</th>\n",
       "      <td>分手了没拉黑不和好咋办 还有机会复合嘛?</td>\n",
       "      <td>可以试试断联。 你们目前的状态非常糟糕，再困在这里不会得到破局的办法，不如断联，给彼此一点时...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34976</th>\n",
       "      <td>为什么有时候明明很饿，却一点也不想吃饭呢？</td>\n",
       "      <td>胃口好不好并不仅仅与饥饿有关，有时候还是“跟着心情走”的。“人的情绪状态就像一个调节器，能放...</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>怎样去找很久都没有聊天的人聊天呢？</td>\n",
       "      <td>找不到一个合适的身份去找他聊天。找不到合适他话题。拿起手机之前千言万语，拿起手机后一个字都不...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>拿到易宝支付的offer 下周入职 同时易车那边也让我复试也在下周而且离家近一些 不知道怎么...</td>\n",
       "      <td>个人觉得易宝支付会更有发展的空间，这是所属行业的趋势所决定的，易宝支付属于互联网支付行业，近...</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>很迷茫没有目标，不知道自己喜欢什么？</td>\n",
       "      <td>首先，迷茫的时候不要发呆。 我时常也有迷茫，高中时同学喊我老年痴呆，那个19岁的年龄我感觉比...</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          zhihu_question  \\\n",
       "36606                               分手了没拉黑不和好咋办 还有机会复合嘛?   \n",
       "34976                              为什么有时候明明很饿，却一点也不想吃饭呢？   \n",
       "1346                                   怎样去找很久都没有聊天的人聊天呢？   \n",
       "1341   拿到易宝支付的offer 下周入职 同时易车那边也让我复试也在下周而且离家近一些 不知道怎么...   \n",
       "2285                                  很迷茫没有目标，不知道自己喜欢什么？   \n",
       "\n",
       "                                            zhihu_answer  zhihu_upvotes  \n",
       "36606  可以试试断联。 你们目前的状态非常糟糕，再困在这里不会得到破局的办法，不如断联，给彼此一点时...              5  \n",
       "34976  胃口好不好并不仅仅与饥饿有关，有时候还是“跟着心情走”的。“人的情绪状态就像一个调节器，能放...             64  \n",
       "1346   找不到一个合适的身份去找他聊天。找不到合适他话题。拿起手机之前千言万语，拿起手机后一个字都不...             26  \n",
       "1341   个人觉得易宝支付会更有发展的空间，这是所属行业的趋势所决定的，易宝支付属于互联网支付行业，近...             46  \n",
       "2285   首先，迷茫的时候不要发呆。 我时常也有迷茫，高中时同学喊我老年痴呆，那个19岁的年龄我感觉比...            418  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.sample(5)[['zhihu_question', 'zhihu_answer', 'zhihu_upvotes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5397, 8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5397, 8)\n",
      "(5191, 8)\n",
      "(1848, 8)\n"
     ]
    }
   ],
   "source": [
    "# deduplication\n",
    "import pandas as pd\n",
    "\n",
    "merged = pd.read_csv('../data/filtered_gpt_4o_mini.csv')\n",
    "print(merged.shape)\n",
    "merged = merged.drop_duplicates(subset=['reddit_message_id'])\n",
    "print(merged.shape)\n",
    "merged = merged.drop_duplicates(subset=['zhihu_question_id'])\n",
    "print(merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jch0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
