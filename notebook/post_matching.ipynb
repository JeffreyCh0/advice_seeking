{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import multiprocess as mp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/emb/zhihu_emb.pkl', 'rb') as f:\n",
    "    raw = pickle.load(f)\n",
    "    zh_sentences = raw[0]\n",
    "    zh_emb = raw[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b29046ce03a49c0b685ff9632b988e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emb_model = BGEM3FlagModel('BAAI/bge-m3',  \n",
    "                       use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pre tokenize: 100%|██████████| 113/113 [00:03<00:00, 35.27it/s]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Inference Embeddings: 100%|██████████| 113/113 [15:11<00:00,  8.06s/it]\n"
     ]
    }
   ],
   "source": [
    "reddit = pd.read_csv('../data/reddit_post.csv')\n",
    "reddit = reddit[[\"message_id\", \"title\", \"message\"]]\n",
    "reddit.columns = [\"message_id\",\"question\", \"detail\"]\n",
    "reddit_questions = reddit['question'].tolist()\n",
    "reddit_details = reddit['detail'].tolist()\n",
    "reddit_sentences = [str(x) + \" \" + str(y) for x, y in zip(reddit_questions, reddit_details)]\n",
    "\n",
    "\n",
    "reddit_emb = emb_model.encode(reddit_sentences)['dense_vecs']\n",
    "\n",
    "with open('../data/emb/reddit_emb.pkl', 'wb') as f:\n",
    "    pickle.dump([reddit_sentences, reddit_emb], f)\n",
    "\n",
    "similarity = reddit_emb @ zh_emb.T\n",
    "\n",
    "top_k = 5\n",
    "list_top_k = []\n",
    "for i in range(similarity.shape[0]):\n",
    "    top_k_idx = np.argsort(similarity[i])[::-1][:top_k]\n",
    "    top_k_sim = similarity[i][top_k_idx]\n",
    "    list_top_k.append([(sim,zh_sentences[j]) for sim, j in zip(top_k_sim,top_k_idx)])\n",
    "\n",
    "reddit['top_1'] = [x[0][1] for x in list_top_k]\n",
    "reddit['top_1_sim'] = [x[0][0] for x in list_top_k]\n",
    "reddit['top_2'] = [x[1][1] for x in list_top_k]\n",
    "reddit['top_2_sim'] = [x[1][0] for x in list_top_k]\n",
    "reddit['top_3'] = [x[2][1] for x in list_top_k]\n",
    "reddit['top_3_sim'] = [x[2][0] for x in list_top_k]\n",
    "reddit['top_4'] = [x[3][1] for x in list_top_k]\n",
    "reddit['top_4_sim'] = [x[3][0] for x in list_top_k]\n",
    "reddit['top_5'] = [x[4][1] for x in list_top_k]\n",
    "reddit['top_5_sim'] = [x[4][0] for x in list_top_k]\n",
    "\n",
    "reddit.to_csv('../data/matched.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ir_top5(question, candidates):\n",
    "        \n",
    "    client = OpenAI(api_key = os.environ['OPENAI_API_KEY'])\n",
    "    user_prompt = f\"# Question:\\n{question}\\n\\n # Candidates:\\nA. {candidates[0]}\\nB. {candidates[1]}\\nC. {candidates[2]}\\nD. {candidates[3]}\\nE. {candidates[4]}\\n\\n\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Given an English question, pick the most similar one from the list of 5 Chinese questions.\"\n",
    "                }\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "            \"name\": \"similar_question_response\",\n",
    "            \"strict\": True,\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                \"response\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The letter corresponding to the most similar question.\",\n",
    "                    \"enum\": [\n",
    "                    \"A\",\n",
    "                    \"B\",\n",
    "                    \"C\",\n",
    "                    \"D\",\n",
    "                    \"E\"\n",
    "                    ]\n",
    "                }\n",
    "                },\n",
    "                \"required\": [\n",
    "                \"response\"\n",
    "                ],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "            }\n",
    "        },\n",
    "        temperature=1,\n",
    "        max_completion_tokens=2048,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "        )\n",
    "    \n",
    "    return json.loads(response.choices[0].message.content)[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 28809/28809 [29:54<00:00, 16.05it/s]  \n"
     ]
    }
   ],
   "source": [
    "# reddit[\"gpt_pick\"] = reddit.apply(lambda x: ir_top5(str(x['question'])+\"\\n\"+str(x['detail']), [x['top_1'], x['top_2'], x['top_3'], x['top_4'], x['top_5']]), axis=1)\n",
    "# reddit[\"gpt_pick_question\"] = reddit.apply(lambda x: x[f\"top_{'_ABCDE'.index(x['gpt_pick'])}\"], axis=1)\n",
    "\n",
    "reddit = pd.read_csv('../data/matched.csv')\n",
    "\n",
    "def process_row(row):\n",
    "    \"\"\"Function to process each row\"\"\"\n",
    "    question_detail = str(row['question']) + \"\\n\" + str(row['detail'])\n",
    "    choices = [row['top_1'], row['top_2'], row['top_3'], row['top_4'], row['top_5']]\n",
    "    gpt_pick = ir_top5(question_detail, choices)\n",
    "    gpt_pick_question = row[f\"top_{'_ABCDE'.index(gpt_pick)}\"]\n",
    "    return gpt_pick, gpt_pick_question\n",
    "\n",
    "def parallel_process(df):\n",
    "    \"\"\"Apply process_row function to each row in parallel with progress tracking\"\"\"\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        results = list(tqdm(pool.imap(process_row, (row for _, row in df.iterrows())), \n",
    "                            total=len(df), desc=\"Processing\"))\n",
    "\n",
    "    df[\"gpt_pick\"], df[\"gpt_pick_question\"] = zip(*results)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "reddit = parallel_process(reddit)\n",
    "\n",
    "reddit.to_csv('../data/matched_gpt_4o_mini.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_check(eng_q, chi_q):\n",
    "        \n",
    "    client = OpenAI(api_key = os.environ['OPENAI_API_KEY'])\n",
    "    user_prompt = f\"# English Question:\\n{eng_q}\\n\\n # Chinese Question:\\n{chi_q}\\n\\n\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Given an English question and a Chinese question, determine whether they are asking the same question.\"\n",
    "                }\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "            \"name\": \"similar_question_response\",\n",
    "            \"strict\": True,\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                \"response\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"Whether the English question and Chinese question are asking the same question.\",\n",
    "                }\n",
    "                },\n",
    "                \"required\": [\n",
    "                \"response\"\n",
    "                ],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "            }\n",
    "        },\n",
    "        temperature=1,\n",
    "        max_completion_tokens=2048,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "        )\n",
    "    \n",
    "    return json.loads(response.choices[0].message.content)[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28809/28809 [28:14<00:00, 17.00it/s]  \n"
     ]
    }
   ],
   "source": [
    "# reddit[\"gpt_sim\"] = reddit.apply(lambda x: sim_check(str(x['question'])+\"\\n\"+str(x['detail']), x['gpt_pick_question']), axis=1)\n",
    "\n",
    "def sim_check_wrapper(args):\n",
    "    \"\"\" Wrapper function for parallel processing \"\"\"\n",
    "    row, sim_check_func = args\n",
    "    return sim_check_func(str(row['question']) + \"\\n\" + str(row['detail']), row['gpt_pick_question'])\n",
    "\n",
    "def parallel_apply(df, func, num_workers=4):\n",
    "    \"\"\" Parallel apply using multiprocessing \"\"\"\n",
    "    with mp.Pool(num_workers) as pool:\n",
    "        results = list(tqdm(pool.imap(sim_check_wrapper, [(row, func) for _, row in df.iterrows()]), total=len(df)))\n",
    "    return results\n",
    "\n",
    "# Assuming reddit is your DataFrame and sim_check is your function\n",
    "reddit[\"gpt_sim\"] = parallel_apply(reddit, sim_check, num_workers=mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.to_csv('../data/matched_gpt_4o_mini.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ratio: 0.2040681731403381\n"
     ]
    }
   ],
   "source": [
    "count_false = reddit[reddit['gpt_sim'] == False].shape[0]\n",
    "count_true = reddit[reddit['gpt_sim'] == True].shape[0]\n",
    "true_ratio = count_true / (count_false + count_true)\n",
    "print(f\"True ratio: {true_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5879, 16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit[reddit['gpt_sim'] == True].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit[reddit['gpt_sim'] == True].to_csv('../data/filtered_gpt_4o_mini.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advise_seeking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
