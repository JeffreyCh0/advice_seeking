{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle   \n",
    "from openai import OpenAI\n",
    "import os\n",
    "import multiprocess as mp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/emb/zhihu_emb.pkl', 'rb') as f:\n",
    "    raw = pickle.load(f)\n",
    "    zh_sentences = raw[0]\n",
    "    zh_emb = raw[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf62a9ad56e4f079f0d5f2e850184e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "# emb_model = BGEM3FlagModel('BAAI/bge-m3',  \n",
    "#                        use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv('../data/reddit_post.csv')\n",
    "reddit = reddit[[\"message_id\", \"title\", \"message\"]]\n",
    "reddit.columns = [\"message_id\",\"question\", \"detail\"]\n",
    "reddit_questions = reddit['question'].tolist()\n",
    "reddit_details = reddit['detail'].tolist()\n",
    "reddit_sentences = [str(x) + \" \" + str(y) for x, y in zip(reddit_questions, reddit_details)]\n",
    "\n",
    "\n",
    "# reddit_emb = emb_model.encode(reddit_sentences)['dense_vecs']\n",
    "\n",
    "# with open('../data/emb/reddit_emb.pkl', 'wb') as f:\n",
    "#     pickle.dump([reddit_sentences, reddit_emb], f)\n",
    "\n",
    "with open('../data/emb/reddit_emb.pkl', 'rb') as f:\n",
    "    raw = pickle.load(f)\n",
    "    reddit_sentences = raw[0]\n",
    "    reddit_emb = raw[1]\n",
    "\n",
    "similarity = reddit_emb @ zh_emb.T\n",
    "\n",
    "with open('../data/emb/similarity.pkl', 'wb') as f:\n",
    "    pickle.dump(similarity, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ir_top5(question, candidates):\n",
    "        \n",
    "    client = OpenAI(api_key = os.environ['OPENAI_API_KEY'])\n",
    "    user_prompt = f\"# Question:\\n{question}\\n\\n # Candidates:\\nA. {candidates[0]}\\nB. {candidates[1]}\\nC. {candidates[2]}\\nD. {candidates[3]}\\nE. {candidates[4]}\\n\\n\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Given an English question, pick the most similar one from the list of 5 Chinese questions.\"\n",
    "                }\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "            \"name\": \"similar_question_response\",\n",
    "            \"strict\": True,\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                \"response\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The letter corresponding to the most similar question.\",\n",
    "                    \"enum\": [\n",
    "                    \"A\",\n",
    "                    \"B\",\n",
    "                    \"C\",\n",
    "                    \"D\",\n",
    "                    \"E\"\n",
    "                    ]\n",
    "                }\n",
    "                },\n",
    "                \"required\": [\n",
    "                \"response\"\n",
    "                ],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "            }\n",
    "        },\n",
    "        temperature=1,\n",
    "        max_completion_tokens=2048,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "        )\n",
    "    \n",
    "    return json.loads(response.choices[0].message.content)[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28809 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "reddit = pd.read_csv('../data/reddit_post.csv')\n",
    "reddit = reddit[[\"message_id\", \"title\", \"message\"]]\n",
    "reddit.columns = [\"message_id\",\"question\", \"detail\"]\n",
    "\n",
    "with open('../data/emb/similarity.pkl', 'rb') as f:\n",
    "    similarity = pickle.load(f)\n",
    "    similarity = similarity.T\n",
    "\n",
    "with open('../data/emb/zhihu_emb.pkl', 'rb') as f:\n",
    "    raw = pickle.load(f)\n",
    "    zh_sentences = raw[0]\n",
    "    zh_emb = raw[1]\n",
    "\n",
    "def process_row(question_detail, choices):\n",
    "    \"\"\"Function to process each row\"\"\"\n",
    "    gpt_pick = ir_top5(question_detail, choices)\n",
    "    gpt_pick_question = choices['ABCDE'.index(gpt_pick)]\n",
    "    return gpt_pick, gpt_pick_question\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "results = []\n",
    "top_k = 5\n",
    "list_top_k = []\n",
    "for i, row in tqdm(enumerate(reddit.itertuples(index=False)), total = reddit.shape[0]): #tqdm(enumerate(reddit.iterrows())):\n",
    "    top_k_idx = np.argsort(similarity[i])[::-1][:top_k]\n",
    "    top_k_sim = similarity[i][top_k_idx]\n",
    "    list_top_k.append([(sim,zh_sentences[j]) for sim, j in zip(top_k_sim,top_k_idx)])\n",
    "    \n",
    "    question_detail = str(row.question) + \"\\n\" + str(row.detail)\n",
    "    choices = [zh_sentences[j] for j in top_k_idx]\n",
    "    gpt_pick, gpt_pick_question = process_row(question_detail, choices)\n",
    "    results.append((gpt_pick, gpt_pick_question))\n",
    "\n",
    "    # without replacement\n",
    "    zh_sentences = np.delete(zh_sentences, top_k_idx)\n",
    "    zh_emb = np.delete(zh_emb, top_k_idx, axis=0)\n",
    "    similarity = np.delete(similarity, top_k_idx, axis=1)\n",
    "\n",
    "reddit['top_1'] = [x[0][1] for x in list_top_k]\n",
    "reddit['top_1_sim'] = [x[0][0] for x in list_top_k]\n",
    "reddit['top_2'] = [x[1][1] for x in list_top_k]\n",
    "reddit['top_2_sim'] = [x[1][0] for x in list_top_k]\n",
    "reddit['top_3'] = [x[2][1] for x in list_top_k]\n",
    "reddit['top_3_sim'] = [x[2][0] for x in list_top_k]\n",
    "reddit['top_4'] = [x[3][1] for x in list_top_k]\n",
    "reddit['top_4_sim'] = [x[3][0] for x in list_top_k]\n",
    "reddit['top_5'] = [x[4][1] for x in list_top_k]\n",
    "reddit['top_5_sim'] = [x[4][0] for x in list_top_k]\n",
    "\n",
    "reddit[\"gpt_pick\"], reddit[\"gpt_pick_question\"] = zip(*results)\n",
    "\n",
    "\n",
    "\n",
    "reddit.to_csv('../data/matched_gpt_4o_mini.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_check(eng_q, chi_q):\n",
    "    api_key = os.environ['OPENAI_API_KEY']\n",
    "    client = OpenAI(api_key = api_key)\n",
    "    user_prompt = f\"# English Question:\\n{eng_q}\\n\\n # Chinese Question:\\n{chi_q}\\n\\n\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Given an English question and a Chinese question, determine whether they are asking the same question.\"\n",
    "                }\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "            \"name\": \"similar_question_response\",\n",
    "            \"strict\": True,\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                \"response\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"Whether the English question and Chinese question are asking the same question.\",\n",
    "                }\n",
    "                },\n",
    "                \"required\": [\n",
    "                \"response\"\n",
    "                ],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "            }\n",
    "        },\n",
    "        temperature=1,\n",
    "        max_completion_tokens=2048,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "        )\n",
    "    \n",
    "    return json.loads(response.choices[0].message.content)[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28809/28809 [08:21<00:00, 57.42it/s] \n"
     ]
    }
   ],
   "source": [
    "# reddit[\"gpt_sim\"] = reddit.apply(lambda x: sim_check(str(x['question'])+\"\\n\"+str(x['detail']), x['gpt_pick_question']), axis=1)\n",
    "\n",
    "reddit = pd.read_csv('../data/matched_gpt_4o_mini.csv')\n",
    "\n",
    "def sim_check_wrapper(args):\n",
    "    \"\"\" Wrapper function for parallel processing \"\"\"\n",
    "    row, sim_check_func = args\n",
    "    return sim_check_func(str(row['question']) + \"\\n\" + str(row['detail']), row['gpt_pick_question'])\n",
    "\n",
    "def parallel_apply(df, func, num_workers=4):\n",
    "    \"\"\" Parallel apply using multiprocessing \"\"\"\n",
    "    with mp.Pool(num_workers) as pool:\n",
    "        results = list(tqdm(pool.imap(sim_check_wrapper, [(row, func) for _, row in df.iterrows()]), total=len(df)))\n",
    "    return results\n",
    "\n",
    "# Assuming reddit is your DataFrame and sim_check is your function\n",
    "reddit[\"gpt_sim\"] = parallel_apply(reddit, sim_check, num_workers=mp.cpu_count())\n",
    "reddit.to_csv('../data/matched_gpt_4o_mini.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ratio: 0.014623193328050114\n"
     ]
    }
   ],
   "source": [
    "# reddit = pd.read_csv('../data/r_matched_gpt_4o_mini.csv')\n",
    "\n",
    "count_false = reddit[reddit['gpt_sim'] == False].shape[0]\n",
    "count_true = reddit[reddit['gpt_sim'] == True].shape[0]\n",
    "true_ratio = count_true / (count_false + count_true)\n",
    "print(f\"True ratio: {true_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2325, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit[reddit['gpt_sim'] == True].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2325, 13)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit[reddit['gpt_sim'] == True].drop_duplicates(subset=['zh_question']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit[reddit['gpt_sim'] == True].to_csv('../data/r_filtered_gpt_4o_mini.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006218\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>url</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>answer_creation_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>怎么说服男朋友买烤箱？</td>\n",
       "      <td>emmmmm，首先想说的是，我买厨房用品一般是不用「说服」的，只是在厨房堆的满满当当的情况下...</td>\n",
       "      <td>357137111</td>\n",
       "      <td>914332816</td>\n",
       "      <td>https://www.zhihu.com/question/357137111/answe...</td>\n",
       "      <td>15</td>\n",
       "      <td>2019-11-28T12:01:22.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>航天从业者是如何看待电视剧《你是我的荣耀》的？</td>\n",
       "      <td>难得有个关于航天的剧，职场情节悬不悬浮，航天设定和细节走不走心？带着放大镜看了前18集，...</td>\n",
       "      <td>475169837</td>\n",
       "      <td>2053313113</td>\n",
       "      <td>https://www.zhihu.com/question/475169837/answe...</td>\n",
       "      <td>4432</td>\n",
       "      <td>2021-08-11T07:26:08.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>如何看待PayPal正式进入中国？</td>\n",
       "      <td>PayPal不仅是美国支付巨头，也是国际支付巨头，目前已开拓全球200多个市场，美国以外的市...</td>\n",
       "      <td>348551037</td>\n",
       "      <td>866103409</td>\n",
       "      <td>https://www.zhihu.com/question/348551037/answe...</td>\n",
       "      <td>127</td>\n",
       "      <td>2019-10-22T09:11:15.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>中金公司交易员月薪八万五是如何做到的？</td>\n",
       "      <td>1、首先，考虑到这位交易员的工作经验，月薪八万五的表述是不正确的：其实是一年的全部薪酬除以1...</td>\n",
       "      <td>545938899</td>\n",
       "      <td>2602363788</td>\n",
       "      <td>https://www.zhihu.com/question/545938899/answe...</td>\n",
       "      <td>450</td>\n",
       "      <td>2022-07-31T13:29:04.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>摇滚乐（金属）给你们带来了什么？</td>\n",
       "      <td>ㄟ( ▔, ▔ )ㄏ哪里带来了什么东西啊，除了找到热爱的东西，也失去了很多。听重型现场像疯子...</td>\n",
       "      <td>361437216</td>\n",
       "      <td>1073541478</td>\n",
       "      <td>https://www.zhihu.com/question/361437216/answe...</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-03-12T05:49:28.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  question                                             answer  \\\n",
       "0              怎么说服男朋友买烤箱？  emmmmm，首先想说的是，我买厨房用品一般是不用「说服」的，只是在厨房堆的满满当当的情况下...   \n",
       "1  航天从业者是如何看待电视剧《你是我的荣耀》的？    难得有个关于航天的剧，职场情节悬不悬浮，航天设定和细节走不走心？带着放大镜看了前18集，...   \n",
       "2        如何看待PayPal正式进入中国？  PayPal不仅是美国支付巨头，也是国际支付巨头，目前已开拓全球200多个市场，美国以外的市...   \n",
       "3      中金公司交易员月薪八万五是如何做到的？  1、首先，考虑到这位交易员的工作经验，月薪八万五的表述是不正确的：其实是一年的全部薪酬除以1...   \n",
       "4         摇滚乐（金属）给你们带来了什么？  ㄟ( ▔, ▔ )ㄏ哪里带来了什么东西啊，除了找到热爱的东西，也失去了很多。听重型现场像疯子...   \n",
       "\n",
       "   question_id   answer_id                                                url  \\\n",
       "0    357137111   914332816  https://www.zhihu.com/question/357137111/answe...   \n",
       "1    475169837  2053313113  https://www.zhihu.com/question/475169837/answe...   \n",
       "2    348551037   866103409  https://www.zhihu.com/question/348551037/answe...   \n",
       "3    545938899  2602363788  https://www.zhihu.com/question/545938899/answe...   \n",
       "4    361437216  1073541478  https://www.zhihu.com/question/361437216/answe...   \n",
       "\n",
       "   upvotes      answer_creation_time  \n",
       "0       15  2019-11-28T12:01:22.000Z  \n",
       "1     4432  2021-08-11T07:26:08.000Z  \n",
       "2      127  2019-10-22T09:11:15.000Z  \n",
       "3      450  2022-07-31T13:29:04.000Z  \n",
       "4        5  2020-03-12T05:49:28.000Z  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"wangrui6/Zhihu-KOL\")\n",
    "\n",
    "data_dict = {}\n",
    "data_dict['question'] = ds['train']['INSTRUCTION']\n",
    "data_dict['answer'] = ds['train']['RESPONSE']\n",
    "for key in json.loads(ds['train']['METADATA'][0]).keys():\n",
    "    data_dict[key] = [json.loads(x)[key] for x in ds['train']['METADATA']]\n",
    "df_train = pd.DataFrame(data_dict)\n",
    "\n",
    "def clean_upvotes(upvote_str):\n",
    "    upvote_str = upvote_str.replace('赞同', '').strip()\n",
    "    if len(upvote_str) > 0:\n",
    "        if \"万\" in upvote_str:\n",
    "            upvote_str = upvote_str.replace('万', '').strip()\n",
    "            return int(float(upvote_str) * 10000)\n",
    "        else:\n",
    "            return int(upvote_str)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "df_train['upvotes'] = df_train['upvotes'].apply(clean_upvotes)\n",
    "df_train['answer_id'] = df_train['answer_id'].astype('int64')\n",
    "df_train['question_id'] = df_train['question_id'].astype('int64')\n",
    "\n",
    "# df_train = df_train[df_train['url'].notna()] # use only url is not NaN\n",
    "print(df_train.shape[0])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>url</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>answer_creation_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [question, answer, question_id, answer_id, url, upvotes, answer_creation_time]\n",
       "Index: []"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check NaN\n",
    "df_train[df_train['question_id'].isin([str(x) for x in drop_ids])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2325\n",
      "2325\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zh_question_id</th>\n",
       "      <th>zh_answer_id</th>\n",
       "      <th>zh_answer</th>\n",
       "      <th>zh_upvotes</th>\n",
       "      <th>zh_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>352160616</td>\n",
       "      <td>911748450</td>\n",
       "      <td>2022.9月20日更新 其实我在2022年一月已经彻底放弃虾皮哈哈哈哈，老子不玩啦。———...</td>\n",
       "      <td>231</td>\n",
       "      <td>https://www.zhihu.com/question/352160616/answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>570091915</td>\n",
       "      <td>2784599789</td>\n",
       "      <td>1，科学饮食，可多摄入富含优质蛋白的食物，例如多吃肉类、鸡蛋以及豆制品等，还需要多吃新鲜的蔬...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.zhihu.com/question/570091915/answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20948066</td>\n",
       "      <td>701579176</td>\n",
       "      <td>如今伴随着耳机大厂商都各自推出带有主动降噪耳机，一时间降噪耳机似乎变成了一种流行趋势。一副主...</td>\n",
       "      <td>334</td>\n",
       "      <td>https://www.zhihu.com/question/20948066/answer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>19849704</td>\n",
       "      <td>232291540</td>\n",
       "      <td>1 12年初到北京，在某小型（却土豪）香港驻京办电影公司工作了三年，期间写了不少东西，却浑浑...</td>\n",
       "      <td>728</td>\n",
       "      <td>https://www.zhihu.com/question/19849704/answer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>373924698</td>\n",
       "      <td>1056437024</td>\n",
       "      <td>现在疫情比较严重，更多的时候都是出不了门，还是有一些好玩的，可以在家或在学校做的线上公益。 ...</td>\n",
       "      <td>7</td>\n",
       "      <td>https://www.zhihu.com/question/373924698/answe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    zh_question_id  zh_answer_id  \\\n",
       "4        352160616     911748450   \n",
       "5        570091915    2784599789   \n",
       "11        20948066     701579176   \n",
       "27        19849704     232291540   \n",
       "34       373924698    1056437024   \n",
       "\n",
       "                                            zh_answer  zh_upvotes  \\\n",
       "4   2022.9月20日更新 其实我在2022年一月已经彻底放弃虾皮哈哈哈哈，老子不玩啦。———...         231   \n",
       "5   1，科学饮食，可多摄入富含优质蛋白的食物，例如多吃肉类、鸡蛋以及豆制品等，还需要多吃新鲜的蔬...           0   \n",
       "11  如今伴随着耳机大厂商都各自推出带有主动降噪耳机，一时间降噪耳机似乎变成了一种流行趋势。一副主...         334   \n",
       "27  1 12年初到北京，在某小型（却土豪）香港驻京办电影公司工作了三年，期间写了不少东西，却浑浑...         728   \n",
       "34  现在疫情比较严重，更多的时候都是出不了门，还是有一些好玩的，可以在家或在学校做的线上公益。 ...           7   \n",
       "\n",
       "                                               zh_url  \n",
       "4   https://www.zhihu.com/question/352160616/answe...  \n",
       "5   https://www.zhihu.com/question/570091915/answe...  \n",
       "11  https://www.zhihu.com/question/20948066/answer...  \n",
       "27  https://www.zhihu.com/question/19849704/answer...  \n",
       "34  https://www.zhihu.com/question/373924698/answe...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered = reddit[reddit['gpt_sim'] == True]\n",
    "print(filtered.shape[0])\n",
    "filtered.head()\n",
    "\n",
    "# merge with train data, left on gpt_pick_question and right on question\n",
    "# leave only the rows that has most upvotes\n",
    "\n",
    "merged = pd.merge(filtered, df_train, left_on='zh_question', right_on='question_id', how='left')\n",
    "merged = merged[merged['upvotes'] == merged.groupby('zh_question')['upvotes'].transform('max')]\n",
    "merged = merged.drop_duplicates(subset=['zh_question'])\n",
    "merged = merged[['zh_question', 'answer_id', 'answer', 'upvotes', 'url']]\n",
    "merged.columns = ['zh_question_id', 'zh_answer_id', 'zh_answer', 'zh_upvotes', 'zh_url']\n",
    "merged['zh_upvotes'] = merged['zh_upvotes'].astype('int64')\n",
    "merged['zh_answer_id'] = merged['zh_answer_id'].astype('int64')\n",
    "# merged = merged[merged['zhihu_upvotes']>0]\n",
    "print(merged.shape[0])\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('../data/zh_q_a.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5397, 8)\n",
      "(5191, 8)\n",
      "(1848, 8)\n"
     ]
    }
   ],
   "source": [
    "# deduplication\n",
    "import pandas as pd\n",
    "\n",
    "merged = pd.read_csv('../data/filtered_gpt_4o_mini.csv')\n",
    "print(merged.shape)\n",
    "merged = merged.drop_duplicates(subset=['reddit_message_id'])\n",
    "print(merged.shape)\n",
    "merged = merged.drop_duplicates(subset=['zhihu_question_id'])\n",
    "print(merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jch0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
